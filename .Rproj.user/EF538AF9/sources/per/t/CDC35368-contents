---
title: "IS NLP assignment"
author: "SILI LIU"
date: "2021/1/2"
output: html_document
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.Introduction
## 1.1 Problem Scoping
Nowadays, cell phone has become an essential thing in people's life. People can use cell phone messages to communicate, but with the development of technology, cell phones will also receive some network marketing activities and even network fraud spam, these messages will make people miss some important messages, thus defeating the purpose of the message for effective communication

## 1.2 Objective of the project
The goal of this project is to build a spam filter by using some mechanical learning algorithms which can effectively classify incoming emails or text messages as "spam" or "ham"

## 1.3 Dataset Description
The dataset, from the Brazilian School of Eletrical and Computer Engineering, is called
SMS Spam Collection v. 1 and contains 5,574 messages in English, real and nonattached, and is tagged according to legitimacy (ham) or spam
Link of dataset:<http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/>

# 2.Data Preprocessing
## 2.1 Check working directory
Check the working directory with `wd`. If it is not the one where your data are located, change it with `setwd`.
```{r echo=TRUE}
getwd()    
```

## 2.2 Load libraries
Load the required libraries.
```{r warning=FALSE,message=FALSE}
library(tm)
library(ggplot2)
library(wordcloud)
library(RWeka)
library(reshape2)
library(SnowballC)
library(NLP) 
library(openNLP) 
library(stringr)
library(caTools)

library(caret)
library(randomForest)
library(e1071)
```

## 2.3 Load dataset
This project is going to use the SMS Spam Collection v. 1, created by Tiago A. Almeida
Once unzipped, the data has a text file "SMSSpamCollection.txt".In this case, using `read.delim` function to import this file
```{r echo=TRUE}
#Read text file
data_text <- read.delim("./smsspamcollection/SMSSpamCollection.txt", header=FALSE, sep="\t",colClasses="character",quote = "",encoding="UTF-8")
```
```{r echo=TRUE}
str(data_text)
head(data_text)
colnames(data_text)
```
Renaming the columns into "Target" and "Content"
```{r echo=TRUE}
colnames(data_text)[colnames(data_text) == 'V1'] <- 'Target'
colnames(data_text)[colnames(data_text) == 'V2'] <- 'Content'
summary(data_text)
```
## 2.4 Create a default document term matrix
We need to find which key words in the content of the message would be considered as spam or ham? Which words appear the most within these words and how often do they occur? We use the `DocumentTermMatrix` function to create a  document term matrix (DTM)

### 2.4.1 Load corpus
```{r echo=TRUE}
corpus <- VCorpus(VectorSource(data_text$Content))
as.character((corpus[[1]])) 
```
### 2.4.2 Cleaning Data and creating the document term matrix
```{r echo=TRUE}


dtm = DocumentTermMatrix(corpus,
                                    control=list(stopwords = T,
                                                 removePunctuation = T, 
                                                 removeNumbers = T,
                                                 stripWhitespace = T,
                                                 stemDocument = T))
dtm
dtm= removeSparseTerms(dtm, 0.999)##Avid sparsity
head(dimnames(dtm)$Terms,10)
tail(dimnames(dtm)$Terms,10)
```
### 2.4.3 Create a document term matrix for SAPM messages
Now we need to create a TDM specifically for SAPM messages
```{r echo=TRUE}
data_text_SPAM <- data_text[which(data_text$Target == 'spam'),]
corpus_SPAM <- VCorpus(VectorSource(data_text_SPAM$Content))
dtm_SPAM = DocumentTermMatrix(corpus_SPAM,
                                    control=list(stopwords = T,
                                                 removePunctuation = T, 
                                                 removeNumbers = T,
                                                 stripWhitespace = T,
                                                 stemDocument = T))
dtm_SPAM= removeSparseTerms(dtm_SPAM, 0.995)##Avid sparsity
```

# 3. Analysis of the data
## 3.1 Word Frequency
List the frequency of word occurrences in the full data
```{r echo=TRUE}
freq=sort(colSums(as.matrix(dtm)),decreasing=TRUE)
head(freq,10)
tail(freq,10)
```
Now we look for the frequency of words appearing in SPAM messages
```{r echo=TRUE}
freq_SPAM=sort(colSums(as.matrix(dtm_SPAM)),decreasing=TRUE)
head(freq_SPAM,60)
tail(freq_SPAM,10)
```

## 3.2 Plotting Word Frequency
The top 10 most common terms and their frequencies are displayed in the bar chart.
```{r echo=TRUE}

high.freq=tail(sort(freq),n=10)
hfp.df=data.frame(names=names(high.freq),freq=high.freq)
ggplot(hfp.df, aes(reorder(names,high.freq),high.freq)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```

For SPAM messages
```{r echo=TRUE}
high.freq_SPAM=tail(sort(freq_SPAM),n=10)
hfp.df_SPAM=data.frame(names=names(high.freq_SPAM),freq=high.freq_SPAM)
ggplot(hfp.df_SPAM, aes(reorder(names,high.freq_SPAM), high.freq_SPAM)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies (SPAM)")
```

## 3.3 Create a word cloud
Presenting the word frequency as a word cloud
```{r echo=TRUE}
set.seed(1234)
wordcloud(words = names(freq), freq = freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

In the SPAM messages data
```{r echo=TRUE}
set.seed(1234)
wordcloud(words = names(freq_SPAM), freq = freq_SPAM, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

# 4. Annotate corpus and find simple patterns
In this part, we use some of the functions in the example to apply to each document in the corpus, using on top of the SPAM information file, we can get the number of appearances of certain keywords inside some SPAM messages that we meet in life

## 4.1 Auxiliary functions

### detectPatternOnDocument
```{r echo=TRUE}
detectPatternOnDocument <- function(doc, pattern) {
  x=as.String(doc)
  res=str_match_all(x,pattern)
  
  dimrow=dim(res[[1]])[1]
  dimcol=dim(res[[1]])[2]
  
  # If there are no rows, no matches have been found
  if (dimrow == 0) {
    return(NA)
  }else{
    if (dimcol > 2){
      # If there are three or more columns, we have to paste all the groups together
      for (i in 1:dimrow) {
        res[[1]][i,2] = paste(res[[1]][i,2:dimcol], collapse = ' ')
      }
    }
    
    # We return all the results found separated by ','
    if (dimcol != 1) {
      result = paste(res[[1]][,2], collapse = ', ')
    }else{
      result = paste(res[[1]][,1], collapse = ', ')
    }
    return(result)
  }
}
```
### detectPatternsInCorpus
```{r echo=TRUE}
detectPatternsInCorpus = function(corpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(corpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
    }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
    }
  return (vallEntities)  
}
```
### countMatchesPerRow
```{r echo=TRUE}
countMatchesPerRow = function (df) {
  entityCountPerFile <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(entityCountPerFile) <- c("File","Count")
  
  for (i in 1:nrow(df)) {
    entityCountPerFile$File[i] = df$File[i]
    entityCountPerFile$Count[i] = length(Filter(Negate(is.na),df[i,2:length(df[i,])]))
    }
  return (entityCountPerFile[entityCountPerFile[2]!=0,])
}
```
### countMatchesPerColumn
```{r echo=TRUE}
countMatchesPerColumn = function (df) {
  entityCountPerPattern <- data.frame(matrix(NA, ncol = 2, 
                                             nrow = length(names(df))-1))
  names(entityCountPerPattern) <- c("Entity","Count")
  
  for (i in 2:length(names(df))) {
    entityCountPerPattern$Entity[i-1] = names(df)[i]
    entityCountPerPattern$Count[i-1] = nrow(subset(df, !is.na(df[i])))
    }
  return (entityCountPerPattern)
  }

```
## 4.2 Result
I have selected some keywords of deceptive information that I often encounter in my daily life. For example, "win", "free", "award", etc.
```{r echo=TRUE}
pattern0=c("win")
pattern0=c(pattern0,"won")
pattern0=c(pattern0,"award[ed]?")
pattern0=c(pattern0,"free")
pattern0=c(pattern0,"Free")

matches0 = detectPatternsInCorpus(corpus_SPAM, pattern0)
countMatchesPerRow(matches0)
countMatchesPerColumn(matches0) 
```

# 5. Model Building
## 5.1 Convert the matrix into a data frame
```{r echo=TRUE}


dataset = as.data.frame(as.matrix(dtm))
colnames(dataset) = make.names(colnames(dataset))
dataset$Target=data_text$Target

prop.table(table(dataset$Target))



```

## 5.2 Splitting the dataset into the Training set and Test set
We split the data in the ratio of 70% training set and 30% test set
```{r echo=TRUE}

set.seed(1234)
split = sample.split(dataset$Target, SplitRatio = 0.7)

train = subset(dataset, split==TRUE)
test = subset(dataset, split==FALSE)

prop.table(table(train$Target))
prop.table(table(test$Target))


train$Target= as.factor(train$Target)
test$Target = as.factor(test$Target )

```

## 5.3 Model Building

### 5.3.1 Random Forest
```{r echo=TRUE}



RF_model =  randomForest(
  Target ~ .,
  data=train
)

predictRF = predict(RF_model, newdata=test)
confusionMatrix(table(predictRF,test$Target))
```
### 5.3.2 Support Vector Machine
```{r echo=TRUE}
SVM_model =  svm(
  Target ~ .,
  data=train
)
predictSVM = predict(SVM_model, newdata=test)
confusionMatrix(table(predictSVM,test$Target))


```
### 5.3.3 Cross Validation Decision Tree (rpart)
```{r echo=TRUE}
fitControl <- trainControl(method = "cv",
                            number = 10,
                           classProbs=TRUE, 
                           summaryFunction=twoClassSummary)
CV_tree <- train(Target ~., train,
                   trControl=fitControl,
                    metric='ROC',
                    method='rpart')
predictTree<- predict(CV_tree, test)
confusionMatrix(predictTree, test$Target)
```

# 6. Conclusion
In this project, 3 different simple mechanical learning models are used. 

The first one is **Random Forest**, `RF_model` is able to accurately classify text messages as ham and spam respectively, with very small class errors (FN/TP and FP/TN) approximately equal to 0, which indicates a very high accuracy (97%) of the model on the observations of the training set. Although `RF_model` performs very well on this dataset because it has the highest accuracy, we also need to be careful because random forests take more time and also there is a risk of overfitting without cross-validation this time.
The second one is **Support Vector Machine**, although with 95.99% accuracy, we may think that the performance is good, but a closer look at the specificity rate of 0.7, compared to **RF_model** 0.84, shows that the SVM model does not perform well compared to Random Forest
The last one is Decision Tree with Cross Validation. uses the `rpart` wrapper. The training set is also divided into 10 parts for cross validation using the `trainControl` function, and the final result is a 91.57% accuracy rate. Although this accuracy is low compared with the previous two models, it can be adopted because the cross validation is used to avoid overfit effectively, and the accuracy of 91.57% is also considered a very high accuracy.

From the above 3 model structures, it can be seen that it is viable to construct filters for spam massage by mechanical learning algorithms. In the same case, such filters can be used in the email domain.  The dataset in this project has 5574 cell phone messages, and with the addition of new data, the prediction model will have more training sets, thus improving the model prediction accuracy.


